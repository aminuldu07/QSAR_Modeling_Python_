{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Set\n",
    "\n",
    "\n",
    "This is the first notebook in a series of Jupyter Notebooks linked to the project titled \"__Developing Mechanism-Based Models for Complex Toxicology Study Endpoints Using Standardized Electronic Submission Data__\".  They should be somewhat ordinal, i.e., should be run in order as each Notebook could depend on results from a previous notebook.  \n",
    "\n",
    "This notebook requires a dependency script, `send.py`, which contains an object that is a live instance to the database and can be queried, etc.  This script requires two variables to be configured, `db_dir` and `send_db_file`.  `db_dir` should point to the directory that contains the SQLlite database and `send_db_file` should be the SQLite database file itself. \n",
    "\n",
    "This notebook will create a training set for hepatotoxicity modeling.  The only input needed is what animal/species to make the training set for.  This could be any controlled terminology for SPECIES in SEND.\n",
    "\n",
    "A training set consists of a set of animals with features which are extracted clinical chemistry results, body weights and animal sex as well as histopathology findings classified as being either liver necrosis, steatosis, or cholestasis.  \n",
    "\n",
    "A data folder will be created for each species and a file called `{species}_training_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import re, os\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose species from controlled terminology\n",
    "\n",
    "Define the variable `species` to be the target species for which to make a training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define species to make a training set\n",
    "# and make a seperate folder to store\n",
    "# all the resulting data\n",
    "\n",
    "species = 'RAT'\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "    \n",
    "species_data = os.path.join('data', species)\n",
    "if not os.path.exists(species_data):\n",
    "    os.mkdir(species_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histopathology classification functions\n",
    "\n",
    "The following functions will classify all the liver results in SEND as cholestasis, steatosis, or necrosis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrex_match(regrex, string: str):\n",
    "    \"\"\" generic function to just max a string of text \"\"\"\n",
    "    pattern = re.compile(regrex, re.IGNORECASE)\n",
    "    match = pattern.search(string)\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_cholestasis(finding: str):\n",
    "    \"\"\" takes a text string from an MI finding and classifies whether or not\n",
    "        the finding can be classified as cholestasis\n",
    "\n",
    "        wikipedia link for Cholestasis: https://en.wikipedia.org/wiki/Cholestasis\n",
    "    \"\"\"\n",
    "    regrex = r'chol(e|o|a)|bil(i|e)'\n",
    "    return regrex_match(regrex, finding)\n",
    "\n",
    "def is_steatosis(finding: str):\n",
    "\n",
    "    \"\"\" takes a text string from an MI finding and classifies whether or not\n",
    "        the finding can be classified as steatosis\n",
    "\n",
    "        wikipedia link for steatosis: https://en.wikipedia.org/wiki/Steatosis\n",
    "    \"\"\"\n",
    "    steatosis_regrex = r'fat|lipid|vacuol|acc|steat|congest'\n",
    "    increased_regrex = r'decreas|lower'\n",
    "    return regrex_match(steatosis_regrex, finding) and not regrex_match(increased_regrex, finding)\n",
    "\n",
    "def is_necrosis(finding: str):\n",
    "    \"\"\" takes a text string from an MI finding and classifies whether or not\n",
    "        the finding can be classified as necrosis\n",
    "\n",
    "        \"\"\"\n",
    "    steatosis_regrex = r'necros|fibros|degen|atroph|apop|deplet'\n",
    "    return regrex_match(steatosis_regrex, finding)\n",
    "\n",
    "\n",
    "def classify_helper(helper_fx, findings):\n",
    "    \"\"\" goes through all the findings and returns if the helper function results in true \"\"\"\n",
    "    for finding in findings:\n",
    "        if helper_fx(finding):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "from functools import partial\n",
    "classify_steatosis = partial(classify_helper, is_steatosis)\n",
    "classify_cholestasis = partial(classify_helper, is_cholestasis)\n",
    "classify_necrosis = partial(classify_helper, is_necrosis)\n",
    "\n",
    "def get_classified_liver_results():\n",
    "    \"\"\" will pull all liver results and classfy them as either necrosis, steatosis, cholestasis \"\"\"\n",
    "    mi = send_db.generic_query('SELECT STUDYID, USUBJID, MISTRESC FROM MI WHERE MISPEC=\"LIVER\"')\n",
    "    mi['STEATOSIS'] = mi.groupby(['STUDYID', 'USUBJID'])['MISTRESC'].transform(classify_steatosis)\n",
    "    mi['CHOLESTASIS'] = mi.groupby(['STUDYID', 'USUBJID'])['MISTRESC'].transform(classify_cholestasis)\n",
    "    mi['NECROSIS'] = mi.groupby(['STUDYID', 'USUBJID'])['MISTRESC'].transform(classify_necrosis)\n",
    "    mi['MISTRESC'] = mi.groupby(['STUDYID', 'USUBJID'])['MISTRESC'].transform(lambda x: ';'.join(x))\n",
    "    return mi.drop_duplicates(['STUDYID', 'USUBJID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment phase filtering functions\n",
    "\n",
    "The following function will filter animals to a specified treatment phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sacrifice_phase(df, phase='treatment'):\n",
    "    \n",
    "    import dateutil.parser\n",
    "    import isodate\n",
    "    \n",
    "    \"\"\"\n",
    "    filters a dataframe containing animals (STUDYID and USUBJID) to only\n",
    "    contain animals that were sacrificed during a specific phase\n",
    "    either, screening recovery or treatment\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    starting_columns = df.columns\n",
    "\n",
    "\n",
    "    ds = send_db.generic_query('SELECT STUDYID, USUBJID, DSDECOD, DSSTDTC '\n",
    "                               'FROM DS WHERE DSDECOD != \"RECOVERY SACRIFICE\"')\n",
    "    ds['SACTIME'] = ds.DSSTDTC.apply(lambda x: dateutil.parser.isoparse(x))\n",
    "\n",
    "    df = df.merge(ds)\n",
    "    dm = send_db.generic_query('SELECT STUDYID, USUBJID, ARMCD FROM DM').merge(df)\n",
    "    ta = send_db.generic_query('SELECT STUDYID, ETCD, ARMCD, lower(EPOCH) as EPOCH FROM TA')\n",
    "\n",
    "    animal_epochs = dm.merge(ta, on=['STUDYID', 'ARMCD'])\n",
    "\n",
    "    se = send_db.generic_query('SELECT STUDYID, USUBJID, ETCD, SESTDTC, SEENDTC FROM SE')\n",
    "\n",
    "    animal_times = animal_epochs.merge(se, on=['STUDYID', 'USUBJID', 'ETCD'])\n",
    "\n",
    "    te = send_db.generic_query('SELECT STUDYID, ETCD, TEDUR FROM TE')\n",
    "    animal_times = animal_times.merge(te, on=['STUDYID', 'ETCD'])\n",
    "\n",
    "    # convert times to date python dates for easier comparison\n",
    "\n",
    "    animal_times['EPOCH_START'] = animal_times.SESTDTC.apply(lambda x: dateutil.parser.isoparse(x))\n",
    "    animal_times['ELEMENT_DUR'] = animal_times.TEDUR.apply(lambda x: isodate.parse_duration(x) if x else x)\n",
    "\n",
    "    # animal_times['EPOCH_END'] = np.nan\n",
    "\n",
    "    # sometimes end times are not populated,\n",
    "    # for these cases in the TE domain contains\n",
    "    # the duration so we can add these start date\n",
    "    # to get the time.\n",
    "\n",
    "    animal_times.loc[animal_times.SEENDTC != '', 'EPOCH_END'] = animal_times.loc[animal_times.SEENDTC != '', 'SEENDTC'].apply(lambda x: dateutil.parser.isoparse(x))\n",
    "    animal_times.loc[animal_times.SEENDTC == '', 'EPOCH_END'] = animal_times.loc[animal_times.SEENDTC == '', 'EPOCH_START'] + pd.to_timedelta(animal_times.loc[animal_times.SEENDTC == '', 'ELEMENT_DUR'])\n",
    "    animal_times = animal_times[animal_times.SACTIME.between(animal_times.EPOCH_START, animal_times.EPOCH_END)]\n",
    "\n",
    "    # now we need to find the trial element that\n",
    "    # a subject was sacrificed within\n",
    "\n",
    "\n",
    "    screening_terms = \"pre.*(tr(ea)?t|dos|test|study|exposure)|acclimat|screen|baseline|allocat|random\"\n",
    "    recovery_terms = \"recovery|post.*(tr(ea)?t|dos|test|study|exposure)\"\n",
    "    treatment_terms = \"tr(ea)?t|dos|test|exposure\"\n",
    "    treatment_terms_not = \"off|non|free|holiday\"\n",
    "\n",
    "\n",
    "    def is_screening(epochs):\n",
    "        return (epochs.str.contains(screening_terms)).all()\n",
    "\n",
    "    def is_recovery(epochs):\n",
    "        return (epochs.str.contains(recovery_terms)).all()\n",
    "\n",
    "    def is_treatment(epochs):\n",
    "        return (epochs.str.contains(treatment_terms) & ~(epochs.str.contains(treatment_terms_not))).all()\n",
    "\n",
    "    if phase == 'screening':\n",
    "        fx = is_screening\n",
    "    elif phase == 'recovery':\n",
    "        fx = is_recovery\n",
    "    else:\n",
    "        fx = is_treatment\n",
    "\n",
    "    # group by study and subject in case there are multiple elements per code\n",
    "    # and remove those that are note\n",
    "    final_animals = animal_times[animal_times.groupby(['STUDYID', 'USUBJID'])['EPOCH'].transform(fx)]\n",
    "    return final_animals.drop_duplicates(['STUDYID', 'USUBJID'])[starting_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab classification functions\n",
    "\n",
    "One function for extracting only numeric results from the LB domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(x):\n",
    "    \"\"\" returns null if x does not contain a valid numeric response,\n",
    "    else it extracts that using a regex pattern \"\"\"\n",
    "    digit_pattern = r'[-+]?([0-9]*\\.[0-9]+|[0-9]+)'\n",
    "    digit_extract = re.search(digit_pattern, str(x))\n",
    "    if digit_extract:\n",
    "        return float(digit_extract.group(0))\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `send.py` there is an object called `send_db` that contains an active connection to the current send database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1895 studies in the SEND db\n"
     ]
    }
   ],
   "source": [
    "from send import send_db\n",
    "\n",
    "# check to make there is a valid connection by counting\n",
    "# the number of studies in the databases\n",
    "\n",
    "num_studies = len(send_db.generic_query('SELECT DISTINCT STUDYID FROM AN'))\n",
    "print(f\"There are {num_studies} studies in the SEND db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95464 RAT out of 170440 records in the SEND DB\n"
     ]
    }
   ],
   "source": [
    "# get a list of animals from the SEND DB\n",
    "# and filter based off of species\n",
    "animals = send_db.get_all_animals()\n",
    "target_animals = animals[animals.SPECIES == species]\n",
    "\n",
    "print(f\"There are {len(target_animals)} {species} out of {len(animals)} records in the SEND DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hive/Anaconda3-2020.02/envs/cheminformatics/lib/python3.7/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29304 RAT\n"
     ]
    }
   ],
   "source": [
    "# the modeling set should only contain\n",
    "# terminal animals (i.e., not recovery animals)\n",
    "# for that, we use the function filter_sacrifice_phase\n",
    "\n",
    "target_animals = filter_sacrifice_phase(target_animals, phase='treatment')\n",
    "\n",
    "print(f\"There are {len(target_animals)} {species}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify studies with control groups \n",
    "\n",
    "Because we normalize the results to control groups, we need to filter out the animals that are in studies that do not have an negative control group that can be easily determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27935 RAT with a valid control group\n"
     ]
    }
   ],
   "source": [
    "# add the specific set codes for these animals\n",
    "setcodes = send_db.generic_query('SELECT STUDYID, USUBJID, SETCD FROM DM')\n",
    "target_animals = target_animals.merge(setcodes, on=['STUDYID', 'USUBJID'])\n",
    "\n",
    "# need to now go through each group by study and identify\n",
    "# the control animals.  We only include animals that\n",
    "# are in a study with a negative control.\n",
    "\n",
    "tx = send_db.generic_query(\"SELECT STUDYID, SETCD, TXVAL FROM TX WHERE TXPARMCD == 'TCNTRL'\")\n",
    "target_animals = target_animals.merge(tx, on=['STUDYID', 'SETCD'], how='left')\n",
    "\n",
    "good_animals = []\n",
    "\n",
    "standAlonesWords = [\"placebo\", \"untreated\", \"sham\"]\n",
    "currentModifiers = [\"negative\", \"saline\", \"peg\", \"vehicle\", \"citrate\", \"dextrose\", \"water\", \"air\"]\n",
    "control_expression = r'|'.join(standAlonesWords + currentModifiers)\n",
    "\n",
    "for study, data in target_animals.groupby('STUDYID'):\n",
    "    if (data.TXVAL.value_counts().shape[0] >= 1) and (\n",
    "    data.TXVAL.str.contains(control_expression, case=False, na=False).any()):\n",
    "        good_animals.append(data)\n",
    "\n",
    "good_animals = pd.concat(good_animals)\n",
    "\n",
    "\n",
    "# add a column to the dataframe that identifies a ra rat as a control or not\n",
    "good_animals.loc[good_animals.TXVAL.str.contains(control_expression, case=False, na=False), 'IS_CONTROL'] = True\n",
    "good_animals.loc[~good_animals.TXVAL.str.contains(control_expression, case=False, na=False), 'IS_CONTROL'] = False\n",
    "\n",
    "\n",
    "print(f'There are {good_animals.shape[0]} {species} with a valid control group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical chemistry results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all the clin chem results from \n",
    "# the LB domain an filter out any results\n",
    "# where we cant extract a numericaly value\n",
    "lb = send_db.generic_query('SELECT STUDYID, '\n",
    "                           'USUBJID, '\n",
    "                           'LBSTRESC, '\n",
    "                           'LBTESTCD, '\n",
    "                           'upper(LBCAT) as LBCAT, '\n",
    "                           'upper(LBSCAT) as LBSCAT, '\n",
    "                           'upper(LBSPEC) as LBSPEC, '\n",
    "                           'LBSTRESU FROM LB')\n",
    "lb = lb[lb.USUBJID.isin(good_animals.USUBJID)]\n",
    "lb.loc[:, 'LBSTRESC'] = lb.LBSTRESC.apply(filter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider each unique LBTESTCD and LBSPEC to be a unique test and take the max response for each animal throughout the study.  For studies that have multiple units for a specific LBTESTCD-LBSPEC pair, we take the unit with the highest amount.  \n",
    "\n",
    "__TODO__: See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hive/Anaconda3-2020.02/envs/cheminformatics/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/hive/Anaconda3-2020.02/envs/cheminformatics/lib/python3.7/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/hive/Anaconda3-2020.02/envs/cheminformatics/lib/python3.7/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "lb = lb[lb.LBSTRESC.notnull()]\n",
    "lb.loc[:, 'LBSTRESC_MAX'] = lb.groupby(['STUDYID', 'USUBJID',  'LBTESTCD', 'LBSPEC'])['LBSTRESC'].transform('max')\n",
    "max_responses = lb.drop_duplicates(['STUDYID', 'USUBJID',  'LBTESTCD', 'LBSPEC', 'LBSTRESC_MAX'])\n",
    "max_responses.loc[max_responses.LBSPEC == '', 'LBSPEC'] = 'UNSPECIFIED'\n",
    "max_responses.loc[:, 'LBTESTCD_SPEC'] = max_responses.LBTESTCD + '-' + max_responses.LBSPEC\n",
    "\n",
    "converted_tests = []\n",
    "\n",
    "\n",
    "for gp, gp_data in max_responses.groupby(['STUDYID', 'LBTESTCD_SPEC']):\n",
    "\n",
    "    # if there are more than one unit\n",
    "    # for a test, take the unit thats most populated\n",
    "    # Studies (at least in RATS confirmed) will have \n",
    "    # one unique unit and the rest blank.  Its safe\n",
    "    # to assume these are all the same.  Otherwise,\n",
    "    # if there are multiple units, take the highest\n",
    "    # and remove the rest. \n",
    "    # TODO: to extend this for all cases.  \n",
    "    if (gp_data.LBSTRESU.value_counts().shape[0] > 1) and (not (gp_data.LBSTRESU == '').any()):\n",
    "\n",
    "        best_unit = gp_data.LBSTRESU.value_counts().index[0]\n",
    "\n",
    "        unit_data = gp_data[gp_data.LBSTRESU == best_unit]\n",
    "        converted_tests.append(unit_data)\n",
    "    else:\n",
    "        converted_tests.append(gp_data)\n",
    "\n",
    "\n",
    "converted_tests = pd.concat(converted_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the LB results to control groups for each lab test. We also add the classified liver results.  We do a left merge when adding because any animal without histopath results can be thought of as no disease.  This is due because often only control and high dose groups will get tested, and if they dont see anything they will not do histopath on the middle groups. \n",
    "\n",
    "__TODO__:\n",
    "Consider not keeping animals without histopath results.  We don't necessarily need more inactive responses....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot the table to put it in wide format as oppost to long format\n",
    "animals_pivot = converted_tests.pivot_table(index=['STUDYID', 'USUBJID'], \n",
    "                                            columns='LBTESTCD_SPEC', \n",
    "                                            values='LBSTRESC_MAX').reset_index()\n",
    "\n",
    "# this line below was necessary \n",
    "# prior to loading it on the hive\n",
    "# i think whatever version of pandas\n",
    "# it must have been fixed\n",
    "# del animals_pivot.columns.name\n",
    "\n",
    "# get classifications for each animal\n",
    "mi = get_classified_liver_results()\n",
    "\n",
    "# identify the columns that contain clinical chemistry tests\n",
    "tests = animals_pivot.columns[~animals_pivot.columns.isin(['STUDYID', 'USUBJID', 'SPECIES', 'SEX', 'IS_CONTROL',\n",
    "                                                           'NECROSIS', 'CHOLESTASIS', 'STEATOSIS'])]\n",
    "\n",
    "animals_with_mi = animals_pivot.merge(mi, how='left').merge(good_animals[['STUDYID', 'USUBJID', 'SPECIES', 'SEX', \n",
    "                                                                          'IS_CONTROL']])\n",
    "\n",
    "for disease in ['NECROSIS', 'CHOLESTASIS', 'STEATOSIS']:\n",
    "    animals_with_mi.loc[animals_with_mi[disease].isnull(), disease] = 0\n",
    "\n",
    "for study, data in animals_with_mi.groupby(['STUDYID', 'SEX']):\n",
    "    control_animals_mean = data[data.IS_CONTROL][tests].mean()\n",
    "    animals_with_mi.loc[data.index, tests] = data[tests].divide(control_animals_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add body weights\n",
    "\n",
    "Create a set of body weight features as well.  These consist of fitting a linear regression line to the body weights throught the study and taking the slope and y interecept, as well as taking a different of the final weight to the animals first weight. \n",
    "\n",
    "These features are also normalized to control and contain `_NORM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = send_db.generic_query('SELECT STUDYID, USUBJID, BWSTRESN, BWSTRESU, BWTESTCD, BWDY FROM BW')\n",
    "animals_bw = animals_with_mi[['STUDYID', 'USUBJID', 'IS_CONTROL', 'SEX']].merge(bw)\n",
    "# # animals_bw = animals_bw[animals_bw.BWSTRESU != '']\n",
    "\n",
    "# # identify control animals for normalization\n",
    "# animals_bw['IS_CONTROL'] = animals_bw['IS_CONTROL'].astype(bool)\n",
    "\n",
    "# these are the functions that will actually create\n",
    "# the features from body weight data\n",
    "def difference_fx(data):\n",
    "    first_weight = data.sort_values(by='BWDY')['BWSTRESN'].iloc[0]\n",
    "    second_weight = data.sort_values(by='BWDY')['BWSTRESN'].iloc[-1]\n",
    "    return second_weight - first_weight\n",
    "\n",
    "def slope_fx(data):\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(data.BWDY, data.BWSTRESN)\n",
    "    return slope\n",
    "\n",
    "def intercept_fx(data):\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(data.BWDY, data.BWSTRESN)\n",
    "    return intercept\n",
    "\n",
    "animals_bw_diff = animals_bw.groupby('USUBJID').apply(difference_fx).reset_index()\n",
    "animals_bw_diff.columns = ['USUBJID', 'BWDIFF']\n",
    "\n",
    "animals_bw = animals_bw.merge(animals_bw_diff)\n",
    "\n",
    "animals_bw_slope = animals_bw.groupby('USUBJID').apply(slope_fx).reset_index()\n",
    "animals_bw_slope.columns = ['USUBJID', 'BWSLOPE']\n",
    "\n",
    "animals_bw = animals_bw.merge(animals_bw_slope)\n",
    "\n",
    "animals_bw_int = animals_bw.groupby('USUBJID').apply(intercept_fx).reset_index()\n",
    "animals_bw_int.columns = ['USUBJID', 'BWINTCEPT']\n",
    "\n",
    "animals_bw = animals_bw.merge(animals_bw_int)\n",
    "\n",
    "new_tests = ['BWDIFF', 'BWSLOPE', 'BWINTCEPT']\n",
    "names = list(map(lambda x: '{}_NORM'.format(x), new_tests))\n",
    "\n",
    "for name in names:\n",
    "    animals_bw[name] = np.nan\n",
    "\n",
    "animals_bw = animals_bw[['STUDYID', 'USUBJID', 'IS_CONTROL', 'SEX'] + new_tests + names].drop_duplicates()\n",
    "animals_bw.index = animals_bw.USUBJID\n",
    "for study, data in animals_bw.groupby(['STUDYID', 'SEX']):\n",
    "    control_animals_mean = data[data.IS_CONTROL][new_tests].mean()\n",
    "\n",
    "    for name in names:\n",
    "        animals_bw.loc[data.index, name] = data[name.replace('_NORM', '')] / control_animals_mean[\n",
    "            name.replace('_NORM', '')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results\n",
    "\n",
    "Merge the final data frame and cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17285 training points.\n"
     ]
    }
   ],
   "source": [
    "final_data = animals_with_mi.merge(animals_bw.reset_index(drop=True).drop(['IS_CONTROL', 'SEX'], axis=1), \n",
    "                                   how='left', on=['STUDYID', 'USUBJID'])\n",
    "training_file = os.path.join(species_data, f'{species}_training_data.csv')\n",
    "final_data.to_csv(training_file)\n",
    "\n",
    "print(f\"There are {final_data.shape[0]} training points.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cheminformatics]",
   "language": "python",
   "name": "conda-env-cheminformatics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
